{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import kagglehub\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                            roc_curve, auc, f1_score, \n",
    "                            precision_score, recall_score)\n",
    "import matplotlib.dates as mdates\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# Check if running in Jupyter\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if 'IPKernelApp' in get_ipython().config:\n",
    "        from tqdm.notebook import tqdm\n",
    "    else:\n",
    "        from tqdm import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea148828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up directories\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "FIGURES_DIR = os.path.join(BASE_DIR, \"figures\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "REPORTS_DIR = os.path.join(BASE_DIR, \"reports\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "# Define color palettes\n",
    "BINARY_PALETTE = ['#3498db', '#e74c3c']\n",
    "MULTI_PALETTE = ['#2C3E50', '#E74C3C', '#3498DB', '#2ECC71', '#F39C12']\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(REPORTS_DIR, 'malware_detection.log')),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    \"\"\"Download the Kaggle dataset\"\"\"\n",
    "    print(\"Downloading dataset...\")\n",
    "    try:\n",
    "        temp_path = kagglehub.dataset_download(\"agungpambudi/network-malware-detection-connection-analysis\")\n",
    "        print(f\"Downloaded to temporary path: {temp_path}\")\n",
    "        \n",
    "        # Move files to the data directory\n",
    "        for file in os.listdir(temp_path):\n",
    "            src_file = os.path.join(temp_path, file)\n",
    "            dst_file = os.path.join(DATA_DIR, file)\n",
    "            shutil.copy(src_file, dst_file)\n",
    "            print(f\"Copied {file} to {dst_file}\")\n",
    "            \n",
    "        print(f\"Dataset files are now available in: {DATA_DIR}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        print(\"Please ensure you have kagglehub installed and have authenticated with Kaggle\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24976a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_name):\n",
    "    \"\"\"\n",
    "    Load a specified dataset file from the data directory\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DATA_DIR, file_name)\n",
    "    print(f\"Loading dataset from: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset with the pipe delimiter\n",
    "        df = pd.read_csv(file_path, delimiter='|', low_memory=False)\n",
    "        \n",
    "        # Clean up the dataset - convert appropriate columns to numeric\n",
    "        numeric_cols = ['duration', 'orig_bytes', 'resp_bytes']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        if 'ts' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['ts'], unit='s')\n",
    "        \n",
    "        # Print basic information about the dataset\n",
    "        print(f\"\\nDataset shape: {df.shape}\")\n",
    "        print(\"\\nColumns in the dataset:\")\n",
    "        print(df.columns.tolist())\n",
    "        print(\"\\nFirst 5 rows of the dataset:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nData types:\")\n",
    "        print(df.dtypes)\n",
    "        print(\"\\nSummary statistics for numeric columns:\")\n",
    "        print(df.describe())\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_values = df.isnull().sum()\n",
    "        print(\"\\nMissing values per column:\")\n",
    "        print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values\")\n",
    "        \n",
    "        # Additional analysis for detailed labels\n",
    "        if 'detailed-label' in df.columns:\n",
    "            print(\"\\nDetailed label distribution:\")\n",
    "            print(df['detailed-label'].value_counts())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf46ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_basic_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform basic analysis on the dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Basic Analysis ===\")\n",
    "    \n",
    "    # Check if 'label' column exists for class distribution analysis\n",
    "    if 'label' in df.columns:\n",
    "        print(\"\\nClass distribution:\")\n",
    "        label_counts = df['label'].value_counts()\n",
    "        print(label_counts)\n",
    "        \n",
    "        # Visualize class distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.countplot(y=df['label'], palette=BINARY_PALETTE)\n",
    "        plt.title('Distribution of Network Traffic Labels', fontweight='bold')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Label')\n",
    "        \n",
    "        # Add count labels to the bars\n",
    "        for i, count in enumerate(label_counts.values):\n",
    "            ax.text(count + 10, i, f\"{count:,} ({count/len(df)*100:.1f}%)\", va='center')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, 'class_distribution.png'))\n",
    "        plt.show()\n",
    "        print(f\"Class distribution plot saved to {os.path.join(FIGURES_DIR, 'class_distribution.png')}\")\n",
    "    \n",
    "    # Check protocol distribution if 'proto' column exists\n",
    "    if 'proto' in df.columns:\n",
    "        print(\"\\nProtocol distribution:\")\n",
    "        proto_counts = df['proto'].value_counts()\n",
    "        print(proto_counts)\n",
    "        \n",
    "        # Visualize protocol distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = sns.countplot(y=df['proto'])\n",
    "        plt.title('Distribution of Network Protocols', fontweight='bold')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Protocol')\n",
    "        \n",
    "        # Add count labels to the bars\n",
    "        for i, count in enumerate(proto_counts.values):\n",
    "            ax.text(count + 10, i, f\"{count:,} ({count/len(df)*100:.1f}%)\", va='center')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, 'protocol_distribution.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Create stacked bar chart showing protocol distribution by label\n",
    "        if 'label' in df.columns:\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            protocol_by_label = pd.crosstab(df['proto'], df['label'], normalize='index') * 100\n",
    "            protocol_by_label.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "            plt.title('Protocol Distribution by Traffic Type', fontweight='bold')\n",
    "            plt.xlabel('Protocol')\n",
    "            plt.ylabel('Percentage')\n",
    "            plt.legend(title='Traffic Type')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add percentage labels\n",
    "            for n, x in enumerate([*protocol_by_label.index.values]):\n",
    "                for (proportion, y_loc) in zip(protocol_by_label.loc[x], protocol_by_label.loc[x].cumsum()):\n",
    "                    if proportion > 5:  # Only show labels for segments > 5%\n",
    "                        plt.text(n, y_loc - proportion/2, f'{proportion:.1f}%', \n",
    "                                ha='center', color='white', fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, 'protocol_by_label.png'))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(df):\n",
    "    \"\"\"\n",
    "    Analyze temporal patterns in the network data\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Temporal Analysis ===\")\n",
    "    \n",
    "    if 'timestamp' in df.columns:\n",
    "        # Add hour of day column\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        \n",
    "        # Traffic volume by hour\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        hourly_traffic = df.groupby(['hour', 'label']).size().unstack()\n",
    "        hourly_traffic.plot(kind='bar', stacked=True)\n",
    "        plt.title('Network Traffic Volume by Hour of Day', fontweight='bold')\n",
    "        plt.xlabel('Hour')\n",
    "        plt.ylabel('Number of Connections')\n",
    "        plt.xticks(range(24), range(24))\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.legend(title='Traffic Type')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, 'hourly_traffic.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Traffic volume time series\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        df['time_bin'] = df['timestamp'].dt.floor('10min')\n",
    "        time_series = df.groupby(['time_bin', 'label']).size().unstack().fillna(0)\n",
    "        \n",
    "        time_series.plot()\n",
    "        plt.title('Network Traffic Volume Over Time', fontweight='bold')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Number of Connections')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(title='Traffic Type')\n",
    "        \n",
    "        # Format x-axis for better readability\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "        plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, 'traffic_timeseries.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_traffic_features(df):\n",
    "    \"\"\"\n",
    "    Analyze network traffic features\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Traffic Feature Analysis ===\")\n",
    "    \n",
    "    # Distribution of bytes transferred\n",
    "    if all(col in df.columns for col in ['orig_bytes', 'resp_bytes']):\n",
    "        # Convert to numeric if needed\n",
    "        for col in ['orig_bytes', 'resp_bytes']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Create total bytes column\n",
    "        df['total_bytes'] = df['orig_bytes'] + df['resp_bytes']\n",
    "        \n",
    "        # Plot distribution of bytes (log scale)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Using log scale due to potential extreme values\n",
    "        for label, color in zip(['Benign', 'Malicious'], BINARY_PALETTE):\n",
    "            data = df[df['label'] == label]['total_bytes'].replace(0, 0.1)  # Replace zeros for log scale\n",
    "            sns.kdeplot(data, shade=True, label=label, color=color, log_scale=True)\n",
    "        \n",
    "        plt.title('Distribution of Data Transfer Volume by Traffic Type (Log Scale)', fontweight='bold')\n",
    "        plt.xlabel('Total Bytes Transferred (log scale)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend(title='Traffic Type')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, 'bytes_distribution.png'))\n",
    "        plt.show()\n",
    "        \n",
    "    # Packets vs Bytes scatter plot\n",
    "    if all(col in df.columns for col in ['orig_pkts', 'orig_bytes']):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create a sample of the data to avoid overcrowding the plot\n",
    "        sample_size = min(10000, len(df))\n",
    "        sample_df = df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        # Create the scatter plot\n",
    "        sns.scatterplot(data=sample_df, x='orig_pkts', y='orig_bytes', hue='label', \n",
    "                        palette=BINARY_PALETTE, alpha=0.6, s=50)\n",
    "        \n",
    "        plt.title('Relationship between Packets and Bytes Sent', fontweight='bold')\n",
    "        plt.xlabel('Number of Packets')\n",
    "        plt.ylabel('Bytes Sent')\n",
    "        plt.legend(title='Traffic Type')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, 'packets_vs_bytes.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ae8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dimensionality_reduction(df):\n",
    "    \"\"\"\n",
    "    Perform PCA on numeric features for visualization\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Dimensionality Reduction Analysis ===\")\n",
    "    \n",
    "    # Select numeric columns for PCA\n",
    "    numeric_cols = ['id.orig_p', 'id.resp_p', 'orig_pkts', 'orig_ip_bytes', \n",
    "                    'resp_pkts', 'resp_ip_bytes']\n",
    "    \n",
    "    # Additional columns if they exist and are numeric\n",
    "    additional_cols = ['duration', 'orig_bytes', 'resp_bytes']\n",
    "    for col in additional_cols:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            numeric_cols.append(col)\n",
    "    \n",
    "    # Check if we have the necessary columns\n",
    "    if all(col in df.columns for col in numeric_cols):\n",
    "        print(f\"Performing PCA on columns: {numeric_cols}\")\n",
    "        \n",
    "        # Sample data to manage memory usage\n",
    "        sample_size = min(50000, len(df))\n",
    "        sample_df = df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        # Extract features and standardize\n",
    "        X = sample_df[numeric_cols].fillna(0)\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        principal_components = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Create a DataFrame with the principal components\n",
    "        pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "        pca_df['label'] = sample_df['label'].reset_index(drop=True)\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='label', palette=BINARY_PALETTE, alpha=0.6, s=50)\n",
    "        \n",
    "        # Add some analytics to the plot\n",
    "        explained_variance = pca.explained_variance_ratio_ * 100\n",
    "        plt.title('PCA of Network Traffic Features', fontweight='bold')\n",
    "        plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2f}% variance explained)')\n",
    "        plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2f}% variance explained)')\n",
    "        plt.legend(title='Traffic Type')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Draw markers to show the distribution of each class\n",
    "        for label, color in zip(['Benign', 'Malicious'], BINARY_PALETTE):\n",
    "            class_data = pca_df[pca_df['label'] == label]\n",
    "            mean_x = class_data['PC1'].mean()\n",
    "            mean_y = class_data['PC2'].mean()\n",
    "            plt.plot(mean_x, mean_y, marker='o', markersize=10, color=color)\n",
    "            plt.annotate(f\"Mean {label}\", (mean_x, mean_y), xytext=(5, 5), \n",
    "                         textcoords='offset points', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, 'pca_visualization.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance based on PCA\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        loadings = pca.components_.T\n",
    "        df_loadings = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=numeric_cols)\n",
    "        \n",
    "        # Plot heatmap of loadings\n",
    "        sns.heatmap(df_loadings, cmap='viridis', annot=True, fmt='.2f', linewidths=0.5)\n",
    "        plt.title('Feature Contributions to Principal Components', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, 'pca_loadings.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file_missing_data(df, file_name):\n",
    "    \"\"\"\n",
    "    Analyze missing data in a single file\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame to analyze\n",
    "    file_name: str, name of the file for reporting\n",
    "    \n",
    "    Returns:\n",
    "    dict: Missing value statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Analyzing {file_name} Missing Data ===\")\n",
    "    \n",
    "    # Get dataset shape\n",
    "    rows, cols = df.shape\n",
    "    print(f\"Dataset shape: {rows} rows x {cols} columns\")\n",
    "    \n",
    "    # Calculate total missing values\n",
    "    total_missing = df.isna().sum().sum()\n",
    "    missing_percentage = (total_missing / (rows * cols)) * 100\n",
    "    print(f\"Total missing values: {total_missing} ({missing_percentage:.2f}%)\")\n",
    "    \n",
    "    # Calculate missing values by column\n",
    "    missing_by_column = df.isna().sum()\n",
    "    missing_cols = missing_by_column[missing_by_column > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(\"\\nMissing values by column:\")\n",
    "        for col, count in missing_cols.items():\n",
    "            col_missing_pct = (count / rows) * 100\n",
    "            print(f\"  {col}: {count} missing values ({col_missing_pct:.2f}%)\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found in any column\")\n",
    "    \n",
    "    # For categorical features, check if there are empty strings or special values\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    empty_strings_by_col = {}\n",
    "    for col in object_cols:\n",
    "        empty_count = df[col].str.strip().eq('').sum()\n",
    "        if empty_count > 0:\n",
    "            empty_strings_by_col[col] = empty_count\n",
    "    \n",
    "    if empty_strings_by_col:\n",
    "        print(\"\\nColumns with empty strings:\")\n",
    "        for col, count in empty_strings_by_col.items():\n",
    "            empty_pct = (count / rows) * 100\n",
    "            print(f\"  {col}: {count} empty strings ({empty_pct:.2f}%)\")\n",
    "    \n",
    "    # Check for zeros in numeric columns that might represent missing values\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    zeros_by_col = {}\n",
    "    for col in numeric_cols:\n",
    "        zero_count = (df[col] == 0).sum()\n",
    "        if zero_count > 0:\n",
    "            zeros_by_col[col] = zero_count\n",
    "    \n",
    "    if zeros_by_col:\n",
    "        print(\"\\nNumeric columns with zero values (potential placeholders):\")\n",
    "        for col, count in zeros_by_col.items():\n",
    "            zero_pct = (count / rows) * 100\n",
    "            print(f\"  {col}: {count} zeros ({zero_pct:.2f}%)\")\n",
    "    \n",
    "    # Visualize missing data\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "    plt.title(f'Missing Data in {file_name}', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'missing_data_{file_name.replace(\".\", \"_\")}.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'file': file_name,\n",
    "        'total_rows': rows,\n",
    "        'total_columns': cols,\n",
    "        'total_missing': total_missing,\n",
    "        'missing_percentage': missing_percentage,\n",
    "        'missing_columns': len(missing_cols),\n",
    "        'missing_by_column': missing_cols.to_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess data for machine learning models\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Data Preprocessing ===\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"Handling missing values...\")\n",
    "    \n",
    "    # For numeric columns: replace NaNs with median\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For categorical columns: replace NaNs with most common value\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Convert categorical variables to numerical\n",
    "    print(\"Converting categorical variables...\")\n",
    "    \n",
    "    # Identify categorical columns to encode\n",
    "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object' and col != 'label']\n",
    "    \n",
    "    # Create empty dictionary to store label encoders\n",
    "    encoders = {}\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_cols:\n",
    "        # For simplicity, we'll use one-hot encoding for columns with few unique values\n",
    "        if df[col].nunique() < 10:\n",
    "            # One-hot encoding\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        else:\n",
    "            # For high cardinality features, use label encoding or skip\n",
    "            if df[col].nunique() < 100:  # Skip if too many unique values\n",
    "                df[col] = df[col].astype('category').cat.codes\n",
    "    \n",
    "    # Prepare features and target\n",
    "    print(\"Preparing features and target...\")\n",
    "    \n",
    "    # Ensure 'label' is binary (0 for 'Benign', 1 for 'Malicious')\n",
    "    if 'label' in df.columns:\n",
    "        df['label'] = df['label'].map({'Benign': 0, 'Malicious': 1})\n",
    "    \n",
    "    # Drop any non-numeric columns that remain\n",
    "    df = df.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    \n",
    "    # Normalize features\n",
    "    print(\"Normalizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Save feature names for later use\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "    np.savez(os.path.join(PROCESSED_DIR, 'processed_data.npz'),\n",
    "             X_train=X_train, X_test=X_test, \n",
    "             y_train=y_train, y_test=y_test,\n",
    "             feature_names=feature_names)\n",
    "    \n",
    "    # Save encoders\n",
    "    joblib.dump(encoders, os.path.join(PROCESSED_DIR, 'encoders.pkl'))\n",
    "    \n",
    "    print(f\"Preprocessed data saved to {os.path.join(PROCESSED_DIR, 'processed_data.npz')}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_names, encoders\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    \"\"\"Load the preprocessed data\"\"\"\n",
    "    print(\"\\n=== Loading Preprocessed Data ===\")\n",
    "    \n",
    "    data_path = os.path.join(PROCESSED_DIR, 'processed_data.npz')\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        \n",
    "        X_train = data['X_train']\n",
    "        X_test = data['X_test']\n",
    "        y_train = data['y_train']\n",
    "        y_test = data['y_test']\n",
    "        features = data['feature_names']\n",
    "        \n",
    "        print(f\"Loaded preprocessed data successfully\")\n",
    "        print(f\"Training set: {X_train.shape[0]} samples with {X_train.shape[1]} features\")\n",
    "        print(f\"Testing set: {X_test.shape[0]} samples with {X_test.shape[1]} features\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        train_classes, train_counts = np.unique(y_train, return_counts=True)\n",
    "        test_classes, test_counts = np.unique(y_test, return_counts=True)\n",
    "        \n",
    "        print(\"\\nClass distribution in training set:\")\n",
    "        for cls, count in zip(train_classes, train_counts):\n",
    "            print(f\"  Class {int(cls)}: {count} samples ({count/len(y_train)*100:.2f}%)\")\n",
    "        \n",
    "        print(\"\\nClass distribution in testing set:\")\n",
    "        for cls, count in zip(test_classes, test_counts):\n",
    "            print(f\"  Class {int(cls)}: {count} samples ({count/len(y_test)*100:.2f}%)\")\n",
    "        \n",
    "        # Load encoders\n",
    "        encoders_path = os.path.join(PROCESSED_DIR, 'encoders.pkl')\n",
    "        encoders = joblib.load(encoders_path) if os.path.exists(encoders_path) else {}\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, features.tolist(), encoders\n",
    "    else:\n",
    "        print(f\"Preprocessed data not found at {data_path}\")\n",
    "        return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1308d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train):\n",
    "    \"\"\"Train a Random Forest classifier\"\"\"\n",
    "    print(\"\\n=== Training Random Forest Classifier ===\")\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, verbose=1, n_jobs=-1)\n",
    "    print(f\"Random Forest parameters: {rf.get_params()}\")\n",
    "    print(\"Fitting Random Forest model...\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    rf.fit(X_train, y_train)\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"Random Forest training completed in {training_time}\")\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(rf, os.path.join(MODELS_DIR, \"random_forest.pkl\"))\n",
    "    print(f\"Random Forest model saved to {os.path.join(MODELS_DIR, 'random_forest.pkl')}\")\n",
    "    \n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, y_train):\n",
    "    \"\"\"Train an XGBoost classifier\"\"\"\n",
    "    print(\"\\n=== Training XGBoost Classifier ===\")\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        verbosity=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    print(f\"XGBoost parameters: {xgb_model.get_params()}\")\n",
    "    print(\"Fitting XGBoost model...\")\n",
    "    \n",
    "    # Create an evaluation set\n",
    "    eval_set = [(X_train, y_train)]\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    xgb_model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=eval_set,\n",
    "        verbose=True\n",
    "    )\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"XGBoost training completed in {training_time}\")\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(xgb_model, os.path.join(MODELS_DIR, \"xgboost.pkl\"))\n",
    "    print(f\"XGBoost model saved to {os.path.join(MODELS_DIR, 'xgboost.pkl')}\")\n",
    "    \n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b971c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train):\n",
    "    \"\"\"Train a Support Vector Machine classifier\"\"\"\n",
    "    print(\"\\n=== Training SVM Classifier ===\")\n",
    "    \n",
    "    # Use a subset of data for SVM due to computational complexity\n",
    "    sample_size = min(10000, X_train.shape[0])\n",
    "    indices = np.random.choice(X_train.shape[0], sample_size, replace=False)\n",
    "    X_train_sample = X_train[indices]\n",
    "    y_train_sample = y_train[indices]\n",
    "    \n",
    "    print(f\"Using {sample_size} samples for SVM training\")\n",
    "    \n",
    "    svm = SVC(kernel='rbf', probability=True, random_state=42, verbose=True)\n",
    "    print(f\"SVM parameters: {svm.get_params()}\")\n",
    "    print(\"Fitting SVM model (this may take some time)...\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    svm.fit(X_train_sample, y_train_sample)\n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"SVM training completed in {training_time}\")\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(svm, os.path.join(MODELS_DIR, \"svm.pkl\"))\n",
    "    print(f\"SVM model saved to {os.path.join(MODELS_DIR, 'svm.pkl')}\")\n",
    "    \n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecaa56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, features=None):\n",
    "    \"\"\"Evaluate a model and create visualizations\"\"\"\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Make predictions\n",
    "    print(f\"Making predictions with {model_name}...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get probabilities (if available)\n",
    "    y_prob = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Save metrics to a CSV file\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1': [f1]\n",
    "    })\n",
    "    \n",
    "    metrics_file = os.path.join(REPORTS_DIR, f\"{model_name.replace(' ', '_').lower()}_metrics.csv\")\n",
    "    metrics_df.to_csv(metrics_file, index=False)\n",
    "    print(f\"Metrics saved to {metrics_file}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Benign', 'Malicious'],\n",
    "                yticklabels=['Benign', 'Malicious'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontweight='bold')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'confusion_matrix_{model_name.replace(\" \", \"_\").lower()}.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance (if available)\n",
    "    if features is not None and hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = model.feature_importances_\n",
    "        # Sort features by importance\n",
    "        sorted_idx = feature_importance.argsort()[-15:]  # top 15 features\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "        plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])\n",
    "        plt.title(f'Feature Importance - {model_name}', fontweight='bold')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f'feature_importance_{model_name.replace(\" \", \"_\").lower()}.png'))\n",
    "        plt.show()\n",
    "    \n",
    "    # Create ROC curve (if probabilities are available)\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name}', fontweight='bold')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f'roc_curve_{model_name.replace(\" \", \"_\").lower()}.png'))\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision, \n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results, y_test):\n",
    "    \"\"\"Compare the performance of different models\"\"\"\n",
    "    print(\"\\n=== Model Performance Comparison ===\")\n",
    "    \n",
    "    # Prepare data for comparison plots\n",
    "    models = list(results.keys())\n",
    "    accuracies = [results[model]['accuracy'] for model in models]\n",
    "    precisions = [results[model]['precision'] for model in models]\n",
    "    recalls = [results[model]['recall'] for model in models]\n",
    "    f1_scores = [results[model]['f1'] for model in models]\n",
    "    \n",
    "    # Create a bar chart comparing all metrics\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    metrics_data = np.array([accuracies, precisions, recalls, f1_scores])\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.2\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.bar(x + i*width, metrics_data[i], width, label=metric, color=colors[i])\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Comparison', fontweight='bold')\n",
    "    plt.xticks(x + width*1.5, models)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on top of the bars\n",
    "    for i, metric_values in enumerate(metrics_data):\n",
    "        for j, value in enumerate(metric_values):\n",
    "            plt.text(j + i*width, value + 0.01, f'{value:.3f}', \n",
    "                     ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'model_metrics_comparison.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC curve comparison\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        if result['y_prob'] is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, result['y_prob'])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, lw=2, color=colors[i % len(colors)],\n",
    "                    label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison', fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'roc_curves_comparison.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a table with performance metrics\n",
    "    metrics_table = pd.DataFrame({\n",
    "        'Accuracy': accuracies,\n",
    "        'Precision': precisions,\n",
    "        'Recall': recalls,\n",
    "        'F1 Score': f1_scores\n",
    "    }, index=models)\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(metrics_table)\n",
    "    \n",
    "    # Find and print the best model for each metric\n",
    "    best_model = {\n",
    "        'Accuracy': models[np.argmax(accuracies)],\n",
    "        'Precision': models[np.argmax(precisions)],\n",
    "        'Recall': models[np.argmax(recalls)],\n",
    "        'F1 Score': models[np.argmax(f1_scores)]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nBest model for each metric:\")\n",
    "    for metric, model in best_model.items():\n",
    "        print(f\"  {metric}: {model}\")\n",
    "    \n",
    "    return metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda951b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalwareDetectionNN(nn.Module):\n",
    "    \"\"\"Neural Network for IoT Malware Detection\"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64, 32], dropout_rate=0.3):\n",
    "        super(MalwareDetectionNN, self).__init__()\n",
    "        \n",
    "        # Input layer\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]),\n",
    "                  nn.BatchNorm1d(hidden_sizes[0]),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),\n",
    "                nn.BatchNorm1d(hidden_sizes[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train, y_train, X_test, y_test, input_size, batch_size=64, \n",
    "                         epochs=30, learning_rate=0.001, patience=7):\n",
    "    \"\"\"Train a neural network for malware detection\"\"\"\n",
    "    print(\"\\n=== Training Neural Network ===\")\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1))\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Create PyTorch datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = MalwareDetectionNN(input_size)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, \n",
    "                                                    patience=5, verbose=True)\n",
    "    \n",
    "    # Training setup\n",
    "    print(f\"Neural Network architecture:\\n{model}\")\n",
    "    print(f\"Training with batch size: {batch_size}, learning rate: {learning_rate}\")\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_acc = train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Track statistics\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss = val_loss / len(test_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    training_time = datetime.now() - start_time\n",
    "    print(f\"Training completed in {training_time}\")\n",
    "    \n",
    "    # Load the best model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), os.path.join(MODELS_DIR, \"neural_network.pth\"))\n",
    "    print(f\"Model saved to {os.path.join(MODELS_DIR, 'neural_network.pth')}\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'neural_network_training_curves.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return model, (train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_neural_network(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the neural network model\"\"\"\n",
    "    print(\"\\n=== Evaluating Neural Network ===\")\n",
    "    \n",
    "    # Convert test data to PyTorch tensors\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        y_prob = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Neural Network Performance:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Save metrics to a CSV file\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': ['Neural Network'],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1': [f1]\n",
    "    })\n",
    "    \n",
    "    metrics_file = os.path.join(REPORTS_DIR, \"neural_network_metrics.csv\")\n",
    "    metrics_df.to_csv(metrics_file, index=False)\n",
    "    print(f\"Metrics saved to {metrics_file}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Benign', 'Malicious'],\n",
    "                yticklabels=['Benign', 'Malicious'])\n",
    "    plt.title('Confusion Matrix - Neural Network', fontweight='bold')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'confusion_matrix_neural_network.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Create ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - Neural Network', fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'roc_curve_neural_network.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision, \n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models(traditional_results, nn_results):\n",
    "    \"\"\"Compare neural network with traditional ML models\"\"\"\n",
    "    print(\"\\n=== Comparing All Models ===\")\n",
    "    \n",
    "    # Combine all results\n",
    "    all_results = traditional_results.copy()\n",
    "    all_results[\"Neural Network\"] = nn_results\n",
    "    \n",
    "    # Compare models\n",
    "    compare_models(all_results, y_test)\n",
    "    \n",
    "    # Create a combined metrics CSV\n",
    "    all_metrics = []\n",
    "    for model_name, results in all_results.items():\n",
    "        all_metrics.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1 Score': results['f1']\n",
    "        })\n",
    "    \n",
    "    all_metrics_df = pd.DataFrame(all_metrics)\n",
    "    all_metrics_file = os.path.join(REPORTS_DIR, \"all_models_comparison.csv\")\n",
    "    all_metrics_df.to_csv(all_metrics_file, index=False)\n",
    "    print(f\"Combined metrics saved to {all_metrics_file}\")\n",
    "    \n",
    "    return all_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\n=====================================================\")\n",
    "    print(\"IOT NETWORK MALWARE DETECTION PIPELINE\")\n",
    "    print(\"=====================================================\")\n",
    "    \n",
    "    # Step 1: Download dataset\n",
    "    dataset_downloaded = download_dataset()\n",
    "    if not dataset_downloaded:\n",
    "        print(\"Proceeding with existing data if available...\")\n",
    "    \n",
    "    # Step 2: List available datasets and choose one to analyze\n",
    "    csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        print(\"\\nAvailable datasets:\")\n",
    "        for i, file in enumerate(csv_files):\n",
    "            print(f\"{i+1}. {file}\")\n",
    "        \n",
    "        # Choose the first dataset for analysis\n",
    "        selected_file = csv_files[0]\n",
    "        print(f\"\\nSelected dataset for analysis: {selected_file}\")\n",
    "        \n",
    "        # Step 3: Load and analyze dataset\n",
    "        df = load_dataset(selected_file)\n",
    "        if df is not None:\n",
    "            # Step 4: Perform data analysis\n",
    "            perform_basic_analysis(df)\n",
    "            analyze_temporal_patterns(df)\n",
    "            analyze_traffic_features(df)\n",
    "            perform_dimensionality_reduction(df)\n",
    "            \n",
    "            # Step 5: Analyze missing data\n",
    "            analyze_file_missing_data(df, selected_file)\n",
    "            \n",
    "            # Step 6: Preprocess data\n",
    "            X_train, X_test, y_train, y_test, features, encoders = preprocess_data(df)\n",
    "        else:\n",
    "            # Try to load preprocessed data if dataset loading failed\n",
    "            X_train, X_test, y_train, y_test, features, encoders = load_preprocessed_data()\n",
    "    else:\n",
    "        print(\"No datasets found. Please download the dataset first.\")\n",
    "        X_train, X_test, y_train, y_test, features, encoders = load_preprocessed_data()\n",
    "    \n",
    "    # Step 7: Train and evaluate models if data is available\n",
    "    if X_train is not None and y_train is not None:\n",
    "        # Train traditional ML models\n",
    "        print(\"\\n=== Training Traditional ML Models ===\")\n",
    "        \n",
    "        rf_model = train_random_forest(X_train, y_train)\n",
    "        xgb_model = train_xgboost(X_train, y_train)\n",
    "        svm_model = train_svm(X_train, y_train)\n",
    "        \n",
    "        # Evaluate traditional ML models\n",
    "        print(\"\\n=== Evaluating Traditional ML Models ===\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if rf_model is not None:\n",
    "            results['Random Forest'] = evaluate_model(rf_model, X_test, y_test, \"Random Forest\", features)\n",
    "        \n",
    "        if xgb_model is not None:\n",
    "            results['XGBoost'] = evaluate_model(xgb_model, X_test, y_test, \"XGBoost\", features)\n",
    "        \n",
    "        if svm_model is not None:\n",
    "            results['SVM'] = evaluate_model(svm_model, X_test, y_test, \"SVM\")\n",
    "        \n",
    "        # Compare traditional ML models\n",
    "        if len(results) > 1:\n",
    "            metrics_table = compare_models(results, y_test)\n",
    "        \n",
    "        # Step 8: Train and evaluate neural network\n",
    "        print(\"\\n=== Training Neural Network Model ===\")\n",
    "        \n",
    "        input_size = X_train.shape[1]\n",
    "        nn_model, training_history = train_neural_network(\n",
    "            X_train, y_train, X_test, y_test, \n",
    "            input_size=input_size,\n",
    "            batch_size=128,\n",
    "            epochs=30,\n",
    "            learning_rate=0.001,\n",
    "            patience=7\n",
    "        )\n",
    "        \n",
    "        # Evaluate neural network\n",
    "        nn_results = evaluate_neural_network(nn_model, X_test, y_test)\n",
    "        \n",
    "        # Step 9: Compare all models\n",
    "        if results:\n",
    "            all_metrics = compare_all_models(results, nn_results)\n",
    "            \n",
    "            print(\"\\n=== Final Model Recommendations ===\")\n",
    "            best_model_f1 = all_metrics.loc[all_metrics['F1 Score'].idxmax(), 'Model']\n",
    "            best_model_accuracy = all_metrics.loc[all_metrics['Accuracy'].idxmax(), 'Model']\n",
    "            \n",
    "            print(f\"Best model by F1 Score: {best_model_f1}\")\n",
    "            print(f\"Best model by Accuracy: {best_model_accuracy}\")\n",
    "            \n",
    "            print(\"\\nRecommendation:\")\n",
    "            print(f\"Based on the results, we recommend using the {best_model_f1} model for IoT malware detection.\")\n",
    "            print(\"This model provides the best balance between precision and recall, crucial for security applications.\")\n",
    "    else:\n",
    "        print(\"Unable to proceed with model training. Please ensure data is properly loaded.\")\n",
    "    \n",
    "    print(\"\\n=== Pipeline Complete ===\")\n",
    "    print(f\"All results and visualizations have been saved in {REPORTS_DIR} and {FIGURES_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
